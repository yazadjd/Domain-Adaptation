# -*- coding: utf-8 -*-
"""Task 2.ipynb

Automatically generated by Colaboratory.

This notebook contains the code relevant to Task 2 of Project 2 - SML Sem 1, 2020.

The CORAL feature transformation algorithm is followed as presented in the paper:

B. Sun, J. Feng, and K. Saenko, “Return of frustratingly easy domain adaptation,” 30th AAAI Conf. Artif. Intell. AAAI 2016, no. 1, pp. 2058–2065, 2016.
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from statistics import mean
import numpy as np
from sklearn import datasets, linear_model
from sklearn import metrics
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from math import sqrt
from operator import itemgetter
from scipy import interpolate
from sklearn.preprocessing import OneHotEncoder 
from sklearn.compose import ColumnTransformer 
from scipy.linalg import sqrtm

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/SML_Proj2

male = pd.read_csv("MALE.csv")
female = pd.read_csv("FEMALE.csv")
mixed = pd.read_csv("MIXED.csv")

male_features  = male[['Year',	'FSM',	'VR1 Band',	'VR Band of Student',	'Ethnic group of student',	'School denomination']]
male_scores = male [['Exam Score']]

male_features.head()

to_encode = ['Year', 'VR Band of Student', 'Ethnic group of student', 'School denomination']

male_features = pd.get_dummies(male_features, prefix_sep = "__", columns = to_encode)

male_features.head()

male_features.shape

#Splitting data for male
X_train, male_X_test, y_train, male_Y_test = train_test_split(male_features, male_scores, test_size = 0.2, random_state = 42)
male_X_train, male_X_val, male_Y_train, male_Y_val = train_test_split(X_train, y_train, test_size = 100, random_state = 42)

#Checking distribution using mean of each partition
print(mean(male_Y_test['Exam Score']))
print(mean(male_Y_train['Exam Score']))
print(mean(male_Y_val['Exam Score']))

female_features  = female[['Year',	'FSM',	'VR1 Band',	'VR Band of Student',	'Ethnic group of student',	'School denomination']]
female_scores = female [['Exam Score']]

female_features = pd.get_dummies(female_features, prefix_sep = "__", columns = to_encode)

#Splitting data for female

X_train, female_X_test, y_train, female_Y_test = train_test_split(female_features, female_scores, test_size = 0.2, random_state = 42)
female_X_train, female_X_val, female_Y_train, female_Y_val = train_test_split(X_train, y_train, test_size = 100, random_state = 42)

#Checking distribution using mean of each partition
print(mean(female_Y_test['Exam Score']))
print(mean(female_Y_train['Exam Score']))
print(mean(female_Y_val['Exam Score']))

mixed.shape

mixed.drop(mixed[mixed['VR Band of Student'] == 0].index, inplace = True)
mixed.reset_index(inplace = True)

mixed.tail()

mixed_features  = mixed[['Year',	'FSM',	'VR1 Band',	'VR Band of Student',	'Ethnic group of student',	'School denomination']]
mixed_scores = mixed[['Exam Score']]

mixed_features = pd.get_dummies(mixed_features, prefix_sep = "__", columns = to_encode)

#Splitting data for mixed

X_train, mixed_X_test, y_train, mixed_Y_test = train_test_split(mixed_features, mixed_scores, test_size = 0.2, random_state = 42)
mixed_X_train, mixed_X_val, mixed_Y_train, mixed_Y_val = train_test_split(X_train, y_train, test_size = 100, random_state = 42)

#Checking distribution using mean of each partition
print(mean(mixed_Y_test['Exam Score']))
print(mean(mixed_Y_train['Exam Score']))
print(mean(mixed_Y_val['Exam Score']))

print(male_features.shape)
print(female_features.shape)
print(mixed_features.shape)

#Reproducible Sampling function to get n instances without replacement
def sample(df):
  return df.sample(n = 100, replace = False, random_state = 42)

"""Transforming Feature Space target = male"""

src =  pd.concat([female_features, mixed_features]).to_numpy()
src_y = pd.concat([female_scores, mixed_scores])
tgt = sample(male_features).to_numpy()

id = np.identity(22)

cov_tgt = np.cov(tgt, rowvar = False) + id
cov_src = np.cov(src, rowvar = False) + id

inv = np.linalg.inv(cov_src)
mid = np.dot(src, sqrtm(inv))

A_CORAL = np.dot(mid, sqrtm(cov_tgt))

A_CORAL.shape

male_features = pd.DataFrame(A_CORAL)

male_features.head()

#Splitting data for male
X_train, male_X_test, y_train, male_Y_test = train_test_split(male_features, src_y, test_size = 0.2, random_state = 42)
male_X_train, male_X_val, male_Y_train, male_Y_val = train_test_split(X_train, y_train, test_size = 100, random_state = 42)

male_X_train.head()

"""TARGET DOMAIN = MALE"""

def lin_reg(x_train, y_train, x_val, y_val):
  regr = linear_model.LinearRegression()
  regr.fit(x_train, y_train)
  lin_pred = regr.predict(x_val)
  print('Mean Absolute Error:', metrics.mean_absolute_error(y_val, lin_pred))
  print('Mean Squared Error:', metrics.mean_squared_error(y_val, lin_pred))
  print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_val, lin_pred)))
  #return lin_pred

#Code to get best value of hyperparameter
all = []
for n in range(120,180):
  all.append((MLP_Reg(x_train, y_train, mixed_X_val, mixed_Y_val, n), n))
print(min(all, key = itemgetter(0))[1])

x_train = male_X_train
y_train = male_Y_train

lin_reg(x_train, y_train, male_X_val, male_Y_val)
lin_reg(x_train, y_train, male_X_test, male_Y_test)

#MLPRegressor(random_state = 42, max_iter = 1000, activation = 'relu', solver = 'adam', hidden_layer_sizes=h, learning_rate='constant', learning_rate_init= 0.001)
MLP_Reg(x_train, y_train, male_X_val, male_Y_val, 101)
MLP_Reg(x_train, y_train, male_X_test, male_Y_test, 101)

"""TARGET DOMAIN = FEMALE

Transforming Feature Space target = female
"""

src =  pd.concat([male_features, mixed_features]).to_numpy()
src_y = pd.concat([male_scores, mixed_scores])
tgt = sample(female_features).to_numpy()

id = np.identity(22)

cov_tgt = np.cov(tgt, rowvar = False) + id
cov_src = np.cov(src, rowvar = False) + id

inv = np.linalg.inv(cov_src)
mid = np.dot(src, sqrtm(inv))

A_CORAL = np.dot(mid, sqrtm(cov_tgt))

A_CORAL.shape

female_features = pd.DataFrame(A_CORAL)

female_features.head()

#Splitting data for female
X_train, female_X_test, y_train, female_Y_test = train_test_split(female_features, src_y, test_size = 0.2, random_state = 42)
female_X_train, female_X_val, female_Y_train, female_Y_val = train_test_split(X_train, y_train, test_size = 100, random_state = 42)

x_train = female_X_train
y_train = female_Y_train

lin_reg(x_train, y_train, female_X_val, female_Y_val)
lin_reg(x_train, y_train, female_X_test, female_Y_test)

def MLP_Reg(x_train, y_train, x_val, y_val, h):
  regr = MLPRegressor(random_state = 42, max_iter = 1000, activation = 'relu', solver = 'adam', hidden_layer_sizes=h, learning_rate='constant', learning_rate_init= 0.001)

  regr.fit(x_train, y_train)
  MLP_pred = regr.predict(x_val)
  print('Mean Absolute Error:', metrics.mean_absolute_error(y_val, MLP_pred))
  print('Mean Squared Error:', metrics.mean_squared_error(y_val, MLP_pred), 'h = ', h)
  print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_val, MLP_pred)))
  return metrics.mean_squared_error(y_val, MLP_pred)
  #return MLP_pred

#MLPRegressor(random_state = 42, max_iter = 1000, activation = 'relu', solver = 'adam', hidden_layer_sizes=h, learning_rate='constant', learning_rate_init= 0.001)
MLP_Reg(x_train, y_train, female_X_val, female_Y_val, 17)
MLP_Reg(x_train, y_train, female_X_test, female_Y_test, 17)

"""TARGET DOMAIN = MIXED"""

src =  pd.concat([male_features, female_features]).to_numpy()
src_y = pd.concat([male_scores, female_scores])
tgt = sample(mixed_features).to_numpy()

id = np.identity(22)

cov_tgt = np.cov(tgt, rowvar = False) + id
cov_src = np.cov(src, rowvar = False) + id

inv = np.linalg.inv(cov_src)
mid = np.dot(src, sqrtm(inv))

A_CORAL = np.dot(mid, sqrtm(cov_tgt))

A_CORAL.shape

mixed_features = pd.DataFrame(A_CORAL)

mixed_features.head()

#Splitting data for mixed

X_train, mixed_X_test, y_train, mixed_Y_test = train_test_split(mixed_features, src_y, test_size = 0.2, random_state = 42)
mixed_X_train, mixed_X_val, mixed_Y_train, mixed_Y_val = train_test_split(X_train, y_train, test_size = 100, random_state = 42)

x_train = mixed_X_train
y_train = mixed_Y_train

lin_reg(x_train, y_train, mixed_X_val, mixed_Y_val)
lin_reg(x_train, y_train, mixed_X_test, mixed_Y_test)

#MLPRegressor(random_state = 42, max_iter = 1000, activation = 'relu', solver = 'adam', hidden_layer_sizes=h, learning_rate='constant', learning_rate_init= 0.001)
MLP_Reg(x_train, y_train, mixed_X_val, mixed_Y_val, 99)
MLP_Reg(x_train, y_train, mixed_X_test, mixed_Y_test, 99)

